{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning Nanodegree Capstone Project\n",
    "# Music Genre Classification\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The problem we are trying to address here is, Can machine Learn Genre of a song by analysing the various feature of songs. Can we match and/or break the break benchmark set by the benchmark model.\n",
    "\n",
    "Based on the research paper <br>\n",
    "_From Classical to Hip-hop: Can Machine Learn Genre? A student publication by Aaron kravitz,Eliza lupone, Ryan Diaz_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "Historically, attempts made by others to build music genre classification systems have yielded fine but not extraordinary results. Some who used the same dataset that we used for our project, which comprises one million songs each belonging to one of ten different genres, often chose to reduce the number of genres they attempted to identify, while those who decided to work with all ten genres only achieved accuracy of 40%. The hardest part of music classification is working with time series data. The cutting edge of machine learning research is looking into more effective ways of extracting features from time series data, but is still a long way off. The most promising attempt is done by Goroshin et. al in their paper Unsupervised Feature Learning from Temporal\n",
    "Data. The problem with temporal data is that the number of features varies per each training example. This makes learning hard, and in particular makes it hard to extract information on exactly what the algorithm is using to “learn”. Algorithms that extract temporal features can be thought of as extracting a more reasonably sized set of\n",
    "features that can be learned from. To address the difficulties of working with temporal data, we used the idea of an acoustic fingerprint to extract a constant sized number of features from each song, improving our accuracy on the testing set substantially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Overview\n",
    "\n",
    "Extracting feature from music file for classification is not that easy. Digital music is discretized sampled\n",
    "waveform, although categorization over frequency domain is preferred than the time domain because\n",
    "of the ability of humans to differentiate between frequencies range like, deep bass in hip-hop is Low\n",
    "frequency while talking, singing is mid-range frequency and sharp claps are considered high-pitched\n",
    "frequency.<br>\n",
    "<br>\n",
    "The base of the project is Million Songs genre dataset that initially contains songs with 34 summary\n",
    "features given as ‘genre, track id, artist name, title, loudness, tempo, time signature, key, mode, duration\n",
    "and 24 different timbre value for each segment’. Each of the song is categorized in one of the 10 genres,\n",
    "mainly classic pop and rock, classical, dance and electronica, folk, pop, hip-hop, jazz and blues, punk,\n",
    "metal & lastly soul and reggae. After that feature scaling has been applied over the dataset to obtain 30\n",
    "summary feature namely ‘loudness, tempo, time signature, key, mode, duration and 24 different timbre\n",
    "value for each segment’ along with the ‘genre’ for each song. The resultant dataset has been modified\n",
    "3 into 9 genre collection by removing hip-hop. Before applying the machine learning algorithms over\n",
    "dataset it has been accessed to obtain almost equal number of features for each genre (approx. 2000\n",
    "songs of each genre) type so as to provide a good training condition for Multiclass Classification\n",
    "algorithms. Some of the main classification algorithms applied are Decision Tree classifier, K Nearest\n",
    "Neighbor classifier, Random Forest and Naïve Bayes classifier, out of which Random Forest has given\n",
    "the best result of 62% F1-Score. The machine learning algorithm has been formulated using scikit learn\n",
    "library and numpy library of python programming.<br><br>\n",
    "Apart from that, the dataset has been modified into 2 summary feature dataset, one being genre and\n",
    "other the lyrics of that particular song, for lyrical modelling approach to be applied over it. Based on\n",
    "the availability of lyrics for each song in the ‘MSD’ dataset for only approx. 24700 songs has been\n",
    "formed that has been re-featured to a collection of approx. 7500 songs in 8 genres. In Lyrical Model\n",
    "the concept of ‘bag of words’ has been implemented for categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets and Inputs\n",
    "\n",
    "For this project the __Million Song Genre Dataset__ has been used, that is been made available by __LabROSA__ at Columbia university , for educational study purpose, Although we will be working with the subset that will be preprocessed by us.This dataset consists of about million songs of various genres, to be precise of 10 genres, namely classic pop and rock , classical , dance and electronica, folk, pop, hip-hop,\n",
    "jazz and blues, punk, metal & lastly soul and reggae .The distribution of songs into various genres can be recognized in Dataset Composition. This dataset is a well-organized classification dataset as it consist the features of each of the song in the dataset too. The Million Song Dataset has about million songs with related metadata and audio analysis features, In this dataset, we have each song as a training example containing 34 summary features along with the genre of the track , which would be of use during training as well as testing the classifiers for predicting the genre of particular combination of features. \n",
    "<br>\n",
    "The dataset contains various __features__ that needs little description.\n",
    "\n",
    "- __genre__ : is the music category of the particular track in the dataset.\n",
    "- __track_id__ : is the musicmatch track id of the track.\n",
    "- __artist_name__ : is the song’s artist name.\n",
    "- __title__ : title of the song.\n",
    "- __loudness__ : is the property of a sound that is primarily a psychological correlation of physical strength (amplitude).\n",
    "- __tempo__ : The tempo is the speed of the underlying beat for a piece of music. Tempo is measured in BPM, or Beats/Minute. The top number represents how many beats there are in a measure, and the bottom number represents the note value which makes one beat.\n",
    "- __key__ : The key of a piece is a group of pitches, or scale upon which a music composition is created.\n",
    "- __mode__ : Refers to a type of scale, coupled with a set of characteristic melodic behaviors.\n",
    "- __duration__ : song duration (in Secs).\n",
    "- __avg_timber1 - 12__ : Timbre is then a general term for the distinguishable characteristics of a tone.\n",
    "- __var_timber1 – 12__ : Timbre is mainly determined by the harmonic content of a sound and the dynamic characteristics.\n",
    "\n",
    "<br>\n",
    "Apart from this, we will be using __Lyrical Modelling concept__ , i.e. to classify genre from the lyrics of the song. We will be collecting available lyrics from the internet and implementing bag of words approach on them.\n",
    "\n",
    "For this we will be scraping lyrics from [Lyrics Wikia](http://lyrics.wikia.com/)\n",
    "\n",
    "\n",
    "We get the preprocessed version of Million Song Dataset and get Million Song Genre Dataset\n",
    "Which can be found and downloaded from this [link](https://labrosa.ee.columbia.edu/millionsong/blog/11-2-28-deriving-genre-dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset and Inputs\n",
    "Loading Million Song Genre Dataset, \n",
    "then taking 2000 samples at most from each available genre so that we get a balanced dataset.\n",
    "\n",
    "Reasons for choosing 2000 samples from each categories:\n",
    "\n",
    "1. This makes our dataset more balanced and doesn't favours one category over above.\n",
    "2. This is done keeping in mind, system resource availability for Storage and Computation of input.\n",
    "\n",
    "\n",
    "In the next block, we load the dataset and show you the composition of dataset. We had around 59000 songs that had genre information available about them. we chose and properly composed our dataset out of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************************\n",
      "No of songs:  59600\n",
      "**************************************************************************\n",
      "Dataset composition:\n",
      "jazz and blues 2000\n",
      "classic pop and rock 2000\n",
      "classical 1874\n",
      "punk 2000\n",
      "metal 2000\n",
      "pop 1617\n",
      "dance and electronica 2000\n",
      "hip-hop 434\n",
      "soul and reggae 2000\n",
      "folk 2000\n",
      "**************************************************************************\n",
      "Loading data in numpy arrays.\n",
      "Time elapsed: 0.324131965637 secs.\n"
     ]
    }
   ],
   "source": [
    "#load and save the dataset and features\n",
    "#Features in extracted from genre_dataset\n",
    "#####################################################################\n",
    "'''\n",
    "Features                        Features Description \n",
    "\n",
    "\n",
    "genre,                          Genre of the song\n",
    "track_id,                       Musicmatch track_id\n",
    "artist_name,                    Artist of the song\n",
    "title,                          Title of the song\n",
    "loudness,                       Loudness is the characteristic of a sound that is primarily a psychological correlate of physical strength (amplitude).\n",
    "tempo,                          The tempo of a piece of music is the speed of the underlying beat. Tempo is measured in BPM, or Beats Per Minute.\n",
    "time_signature,                 The top number represents how many beats there are in a measure, and the bottom number represents the note value which makes one beat.\n",
    "key,                            The key of a piece is a group of pitches, or scale upon which a music composition is created.\t\n",
    "mode,                           Refers to a type of scale, coupled with a set of characteristic melodic behaviours. \n",
    "duration,                       song duration (in Secs)\n",
    "avg_timbre1,                    Timbre is then a general term for the distinguishable characteristics of a tone.\n",
    "avg_timbre2,                    Timbre is mainly determined by the harmonic content of a sound \n",
    "avg_timbre3,                    and the dynamic characteristics.\n",
    "avg_timbre4,\n",
    "avg_timbre5,\n",
    "avg_timbre6,\n",
    "avg_timbre7,\n",
    "avg_timbre8,\n",
    "avg_timbre9,\n",
    "avg_timbre10,\n",
    "avg_timbre11,\n",
    "avg_timbre12,\n",
    "var_timbre1,\n",
    "var_timbre2,\n",
    "var_timbre3,\n",
    "var_timbre4,\n",
    "var_timbre5,\n",
    "var_timbre6,\n",
    "var_timbre7,\n",
    "var_timbre8,\n",
    "var_timbre9,\n",
    "var_timbre10,\n",
    "var_timbre11,\n",
    "var_timbre12\n",
    "'''\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "#                               ABOUT FILE\n",
    "#\n",
    "#File Contains code for Extracting Information from the \"msd_genre_dataset.txt\" file\n",
    "#that contains all the features and labels that will be helpful in \n",
    "#predicting genres.\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "#\n",
    "#                     Library Import Statements\n",
    "#\n",
    "#\n",
    "######################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import matplotlib\n",
    "import time\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "######################################################################################\n",
    "#\n",
    "#                              HELPER FUNCTIONS\n",
    "#\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def processItem(datapoint):\n",
    "    info=[datapoint[0]]\n",
    "    for feature in datapoint[4:]:\n",
    "        info.append(float(feature))\n",
    "    return info\n",
    "\n",
    "\n",
    "\n",
    "data_list=[]\n",
    "\n",
    "genres=[\"jazz and blues\",\"classic pop and rock\",\"classical\",\"punk\",\"metal\",\"pop\",\"dance and electronica\",\"hip-hop\",\"soul and reggae\",\"folk\"]\n",
    "def preprocess():\n",
    "    #open msd_genre_dataset.txt in read mode\n",
    "\n",
    "    line_num=0  #line number in text file\n",
    "    #################################################################\n",
    "    '''\n",
    "    Steps\n",
    "    1. skip 10 lines as no useful data is there.\n",
    "    2. Now, Read line by line and store that info in appropriate data structure for further analysis \n",
    "    '''\n",
    "    ############################################################\n",
    "    '''\n",
    "    The with statement handles opening and closing the file, \n",
    "    including if an exception is raised in the inner block. \n",
    "    The for line in f treats the file object f as an iterable, \n",
    "    which automatically uses buffered IO and memory management \n",
    "    so you don't have to worry about large files.\n",
    "    '''\n",
    "    ############################################################\n",
    "    genres={}\n",
    "\n",
    "    with open('msd_genre_dataset.txt','r') as f:\n",
    "        for line in f:\n",
    "            if line_num<10:\n",
    "                pass\n",
    "            else:\n",
    "                temp=line.split(',')\n",
    "                try:\n",
    "                    if(genres[temp[0]] and genres[temp[0]]<2000):\n",
    "\n",
    "                        genres[temp[0]]+=1\n",
    "                        data_list.append(processItem(temp))\n",
    "                except:\n",
    "                    genres[temp[0]]=1\n",
    "            line_num+=1\n",
    "    print \"**************************************************************************\"\n",
    "    print \"No of songs: \",line_num-10\n",
    "    print \"**************************************************************************\"\n",
    "\n",
    "    print \"Dataset composition:\"\n",
    "    for type in genres.keys():\n",
    "        #print \"\\\"\"+ type+\"\\\",\",\n",
    "        print type,genres[type]\n",
    "    print \"**************************************************************************\"\n",
    "    print \"Loading data in numpy arrays.\"\n",
    "    t=time.time()\n",
    "    np_data=np.array(data_list)\n",
    "    print \"Time elapsed:\",time.time()-t,\"secs.\"\n",
    "    t=time.time()\n",
    "\n",
    "preprocess()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and Labels Split \n",
    "Now, the data we just loaded contains both the feature and labels altogether. So, we need to seperate them out.\n",
    "This is what we are doing in the next block of code. Just loaded data into features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n",
    "target,features = targetFeatureSplit(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot Encoding the Categorial Labels\n",
    "Machine learning algorithms cannot work with categorical data directly. Categorical data must be \n",
    "converted to numbers so that computer can categorise them accordingly .So, we will encode all the labels using LabelEncoder and transform our labels to numerical encoding.\n",
    "\n",
    "These can be inversed transform whenever required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classic pop and rock\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#label of the first song before encoding\n",
    "print target[0]\n",
    "enc = LabelEncoder()\n",
    "target = enc.fit_transform(target)\n",
    "#label of the song after LabelEncoding\n",
    "print target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocessing Data\n",
    "Now, that we have properly encoded our labels. Next step, that we should take is feature scaling. We have 30 features, all these features have different ranges of value, this can cause different feature to have a different weight being used to fit the model.\n",
    "\n",
    "To avoid this, we require to feature scale the values of all the features available to us so that all values come to certain range and feature weight is equally balanced.   \n",
    "\n",
    "We need to feature scale all the features, we just extracted out of million song genre dataset using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71616261  0.5698089   0.14285714  0.36363636  0.          0.06578349\n",
      "  0.70781667  0.30157788  0.54239591  0.51226928  0.34541314  0.43352076\n",
      "  0.70267668  0.49087988  0.62865809  0.44089158  0.4746716   0.48106248\n",
      "  0.07390126  0.03032639  0.10400228  0.03627104  0.03346692  0.07060975\n",
      "  0.08077381  0.04852299  0.09597886  0.09277362  0.04458085  0.02255567]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "features=scaler.fit_transform(features)\n",
    "#sample output\n",
    "print features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data in Numpy arrays for processing\n",
    "Loading target and feature list to numpy arrays, so that can be processed by machine Learning algorithms properly.\n",
    "\n",
    "__Note__:<br>\n",
    "Numpy array expects data to be of one format, they don't expect strings, or NaN entries in them. We need to take care of these things while loading our inputs in numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "[ 0.71616261  0.5698089   0.14285714  0.36363636  0.          0.06578349\n",
      "  0.70781667  0.30157788  0.54239591  0.51226928  0.34541314  0.43352076\n",
      "  0.70267668  0.49087988  0.62865809  0.44089158  0.4746716   0.48106248\n",
      "  0.07390126  0.03032639  0.10400228  0.03627104  0.03346692  0.07060975\n",
      "  0.08077381  0.04852299  0.09597886  0.09277362  0.04458085  0.02255567]\n"
     ]
    }
   ],
   "source": [
    "features = np.array(features,dtype = np.float64)\n",
    "print type(features)\n",
    "print features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split \n",
    "Now, that we have done preprocessing on our labels and features.\n",
    "We need to split our input into training and testing set. Generally training and testing sets are in the ratio of __80:20__. <br>\n",
    "We used train_test_split function to split our input into training and testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadInput():\n",
    "    print \"Creating Training set and Test Set.\"\n",
    "    print \"-----------------------------------\"\n",
    "    print \"\"\n",
    "    t=time.time()\n",
    "    feature_train,feature_test,label_train,label_test = train_test_split(features, target, test_size=0.20, random_state=42)\n",
    "    print \"Time elapsed:\",time.time()-t,\"secs\"\n",
    "    print \"\"\n",
    "\n",
    "    #size of train and test sets\n",
    "    print \"Size of training set:{} samples\".format(len(feature_train))\n",
    "    print \"Size of testing set:{} samples\".format(len(feature_test))\n",
    "    \n",
    "    print \"\"\n",
    "    print \"\"\n",
    "    return feature_train,feature_test,label_train,label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms and Genre Classification task.\n",
    "\n",
    "We will be using different classification algorithms for this multiclass classification task of classifying\n",
    "songs into different genres.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "Generally people use accuracy score to evaluate the performance of a model. But this is a case of multiclass classification, accuracy metric doesn't make sense here since, the number of true positives of any class can \n",
    "vary, another reason of choosing F1-score as evaluation metric is number of data_points of each label can vary a\n",
    "lot. So, F1-score is your best bet for evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Algorithm\n",
    "\n",
    "We started with Naive Bayes Algorithm, reason being it is the simplest to code and understand algorithm.\n",
    "We feed our data to Naive bayes model, the F1-sore it provides our lower bound performance benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Training set and Test Set.\n",
      "-----------------------------------\n",
      "\n",
      "Time elapsed: 0.00357890129089 secs\n",
      "\n",
      "Size of training set:14332 samples\n",
      "Size of testing set:3583 samples\n",
      "\n",
      "\n",
      "Time elapsed: 0.0134110450745 secs.\n",
      "confusion_matrix:\n",
      "\n",
      "[[ 54   8   7  76  17  17  49  78  37  44]\n",
      " [  9 273  14  24   5  13  13   4   5   2]\n",
      " [ 24  55  94  18  43  34  31  60  19  44]\n",
      " [ 31  15  11 162   1  21  22  76  33  31]\n",
      " [  1   0   5   1  27   1   1   8   5  29]\n",
      " [ 30  59  38  41   7 125  15  39  16  20]\n",
      " [  4   6  11   3   1   5 368   8  24   1]\n",
      " [ 15   3   4  45   6  11  37 136  24  31]\n",
      " [ 12   8   7  16  10  14 235  22  57  33]\n",
      " [ 27   2  12  34  30  10  10  83  13 163]]\n",
      "\n",
      "\n",
      "classification_report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.14      0.18       387\n",
      "          1       0.64      0.75      0.69       362\n",
      "          2       0.46      0.22      0.30       422\n",
      "          3       0.39      0.40      0.39       403\n",
      "          4       0.18      0.35      0.24        78\n",
      "          5       0.50      0.32      0.39       390\n",
      "          6       0.47      0.85      0.61       431\n",
      "          7       0.26      0.44      0.33       312\n",
      "          8       0.24      0.14      0.18       414\n",
      "          9       0.41      0.42      0.42       384\n",
      "\n",
      "avg / total       0.40      0.41      0.38      3583\n",
      "\n",
      "\n",
      "\n",
      "Encoded Genres:\n",
      "classic pop and rock :  0\n",
      "classical :  1\n",
      "dance and electronica :  2\n",
      "folk :  3\n",
      "hip-hop :  4\n",
      "jazz and blues :  5\n",
      "metal :  6\n",
      "pop :  7\n",
      "punk :  8\n",
      "soul and reggae :  9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "feature_train,feature_test,label_train,label_test = loadInput()\n",
    "\n",
    "clf_nb = GaussianNB()\n",
    "\n",
    "t = time.time()\n",
    "clf_nb.fit(feature_train,label_train)\n",
    "\n",
    "pred_nb = clf_nb.predict(feature_test)\n",
    "\n",
    "print \"Time elapsed:\",time.time()-t,\"secs.\"\n",
    "print \"confusion_matrix:\"\n",
    "print \"\"\n",
    "print(confusion_matrix(label_test,pred_nb))\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "\n",
    "print \"classification_report:\"\n",
    "print classification_report(label_test,pred_nb)\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "\n",
    "print \"Encoded Genres:\" \n",
    "for index,genre in enumerate(enc.classes_):\n",
    "    print genre,\": \",index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interim Observation\n",
    "\n",
    "We get an F1-score of 41%.This becomes our lower bound.\n",
    "Confusion Matrix above shows us all the false postives, true negetives, false negetives and true positives.\n",
    "\n",
    "Now, the way to improve this F1-score we  move on to Support Vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support  Vector Machine\n",
    "After working out with Naive bayes Algorithm, we came to support vector machine beacause Non-linear SVM algorithms calculates boundaries that don't have to be a straight line. The benefit is that you can capture much more complex relationships between your datapoints without having to perform difficult transformations on your own. The downside is that the training time is much longer as it's much more computationally intensive.\n",
    "\n",
    "So, Naivest SVC model with __'rbf'__ kernel gives us F1-Score of 43%.\n",
    "\n",
    "As, stated SVM are computationally expensive, it took almost 3 times the time to train an SVM than a Naive Bayes Algorithm and still F1-score didn't go up by much. So, we need to look for other complex classification algorithms that can perform better on multiclass classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating Training set and Test Set.\n",
      "-----------------------------------\n",
      "\n",
      "Time elapsed: 0.00397300720215 secs\n",
      "\n",
      "Size of training set:14332 samples\n",
      "Size of testing set:3583 samples\n",
      "\n",
      "\n",
      "Time elapsed: 28.6036570072 secs.\n",
      "confusion_matrix:\n",
      "\n",
      "[[ 89   9   8 109   0  26  25  22  34  65]\n",
      " [  9 285   9  23   0  11   9   7   4   5]\n",
      " [ 29  43 138  40   0  48  23  30  13  58]\n",
      " [ 59  10   3 202   0  33   2  35  26  33]\n",
      " [  1   0   9   4   0   1   3   4   2  54]\n",
      " [ 44  64  36  52   0 138   6  10   6  34]\n",
      " [ 14   5   8   8   0   3 328   7  55   3]\n",
      " [ 75   2   7  61   0   9  12  60  27  59]\n",
      " [ 22  12  11  21   0   6 121  20 153  48]\n",
      " [ 57   2  19  44   0  18   7  30  10 197]]\n",
      "\n",
      "\n",
      "classification_report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.23      0.23       387\n",
      "          1       0.66      0.79      0.72       362\n",
      "          2       0.56      0.33      0.41       422\n",
      "          3       0.36      0.50      0.42       403\n",
      "          4       0.00      0.00      0.00        78\n",
      "          5       0.47      0.35      0.40       390\n",
      "          6       0.61      0.76      0.68       431\n",
      "          7       0.27      0.19      0.22       312\n",
      "          8       0.46      0.37      0.41       414\n",
      "          9       0.35      0.51      0.42       384\n",
      "\n",
      "avg / total       0.44      0.44      0.43      3583\n",
      "\n",
      "\n",
      "\n",
      "Encoded Genres:\n",
      "classic pop and rock :  0\n",
      "classical :  1\n",
      "dance and electronica :  2\n",
      "folk :  3\n",
      "hip-hop :  4\n",
      "jazz and blues :  5\n",
      "metal :  6\n",
      "pop :  7\n",
      "punk :  8\n",
      "soul and reggae :  9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "feature_train,feature_test,label_train,label_test = loadInput()\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "clf_svm = SVC()\n",
    "clf_svm.fit(feature_train, label_train) \n",
    "\n",
    "pred_svm= clf_svm.predict(feature_test)\n",
    "\n",
    "print \"Time elapsed:\",time.time()-t,\"secs.\"\n",
    "\n",
    "print \"confusion_matrix:\"\n",
    "print \"\"\n",
    "print(confusion_matrix(label_test,pred_svm))\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "\n",
    "print \"classification_report:\"\n",
    "print classification_report(label_test,pred_svm)\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "\n",
    "print \"Encoded Genres:\" \n",
    "for index,genre in enumerate(enc.classes_):\n",
    "    print genre,\": \",index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbour Algorithm\n",
    "After Support Vector machine, we though of try something different. kNN is slow when you have a lot of observations, since it does not generalize over data in advance, it scans historical database each time a prediction is needed.\n",
    "\n",
    "With kNN you need to think carefully about the distance measure. For instance, if one feature is measured in 1000s of kilometers, another feature in 0.001 grams, the first feature will dominate the distance measure. You can normalize the features, or give certain importance weights, based on the domain knowledge.\n",
    "\n",
    "Also, in a very high dimensional space the distance to all neighbors becomes more or less the same, and the notion of nearest and far neighbors becomes blurred.\n",
    "\n",
    "The simplest KNN model give us a F1-score of 41%, which comparable to our lower bound.So, we move on our next classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 1.03407907486 secs.\n",
      "confusion_matrix:\n",
      "\n",
      "[[193   7  21  68   3  32  14  26  14   9]\n",
      " [ 24 290  14  12   2   7   2   4   5   2]\n",
      " [ 75  49 146  45   9  33  21  28   7   9]\n",
      " [121  20  15 181   3  18   3  35   6   1]\n",
      " [ 15   5  11   9  16   2   1   8   5   6]\n",
      " [ 83  71  34  60   3  99  10  18   5   7]\n",
      " [ 29  10  15  21   2   3 318   8  24   1]\n",
      " [ 78  12  19  67   5  15  15  85   5  11]\n",
      " [ 63  15  30  35  11  12 100  26 108  14]\n",
      " [118   4  37  57  19  41   4  26  14  64]]\n",
      "\n",
      "\n",
      "classification_report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.24      0.50      0.33       387\n",
      "          1       0.60      0.80      0.69       362\n",
      "          2       0.43      0.35      0.38       422\n",
      "          3       0.33      0.45      0.38       403\n",
      "          4       0.22      0.21      0.21        78\n",
      "          5       0.38      0.25      0.30       390\n",
      "          6       0.65      0.74      0.69       431\n",
      "          7       0.32      0.27      0.30       312\n",
      "          8       0.56      0.26      0.36       414\n",
      "          9       0.52      0.17      0.25       384\n",
      "\n",
      "avg / total       0.45      0.42      0.41      3583\n",
      "\n",
      "\n",
      "\n",
      "Encoded Genres:\n",
      "classic pop and rock :  0\n",
      "classical :  1\n",
      "dance and electronica :  2\n",
      "folk :  3\n",
      "hip-hop :  4\n",
      "jazz and blues :  5\n",
      "metal :  6\n",
      "pop :  7\n",
      "punk :  8\n",
      "soul and reggae :  9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "t=time.time()\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=2)\n",
    "clf_knn.fit(feature_train, label_train) \n",
    "\n",
    "pred_knn = clf_knn.predict(feature_test)\n",
    "\n",
    "print \"Time elapsed:\",time.time()-t,\"secs.\"\n",
    "\n",
    "print \"confusion_matrix:\"\n",
    "print \"\"\n",
    "print(confusion_matrix(label_test,pred_knn))\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "\n",
    "print \"classification_report:\"\n",
    "print classification_report(label_test,pred_knn)\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "\n",
    "print \"Encoded Genres:\" \n",
    "for index,genre in enumerate(enc.classes_):\n",
    "    print genre,\": \",index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Algorithm, Benchmark Model\n",
    "We finally came to Random Forest Algorithm, that has chosen as our benchmark model based on the reseach paper we are working with. The results in paper show that when value of n_estimators is set to/around 300 it gives the best\n",
    "F1-Score.\n",
    "\n",
    "Benchmark Model in the paper suggested it got a F1-Score of 58%. But We got consistent __60% F1-score__.\n",
    "Though, we are working only on 1% of actual dataset, on which F1-score of 58% was achieved.\n",
    "\n",
    "This improvement in the F1-Score can be because of:\n",
    "\n",
    "    - The dataset composition is well balanced for genre categories.\n",
    "    - Proper preprocessing done on data before fitting the model.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Training set and Test Set.\n",
      "-----------------------------------\n",
      "\n",
      "Time elapsed: 0.00345301628113 secs\n",
      "\n",
      "Size of training set:14332 samples\n",
      "Size of testing set:3583 samples\n",
      "\n",
      "\n",
      "Time elapsed: 24.9928629398 secs.\n",
      "confusion_matrix:\n",
      "\n",
      "[[151   9  12  62   3  28  14  45  27  36]\n",
      " [  6 301  15  12   0  15   5   2   2   4]\n",
      " [ 11  21 253  17   3  41  11  18  14  33]\n",
      " [ 33   9   5 237   0  32   2  47  13  25]\n",
      " [  1   0  10   3  24   1   1   3   2  33]\n",
      " [ 28  35  35  29   1 221   2   8   6  25]\n",
      " [ 15   4   9   6   0   4 348   5  38   2]\n",
      " [ 34   0  17  46   2  12   6 154  10  31]\n",
      " [ 20   4  20  19   4  11  32  19 248  37]\n",
      " [ 44   2  22  28   1  21   0  38   4 224]]\n",
      "\n",
      "\n",
      "classification_report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.39      0.41       387\n",
      "          1       0.78      0.83      0.81       362\n",
      "          2       0.64      0.60      0.62       422\n",
      "          3       0.52      0.59      0.55       403\n",
      "          4       0.63      0.31      0.41        78\n",
      "          5       0.57      0.57      0.57       390\n",
      "          6       0.83      0.81      0.82       431\n",
      "          7       0.45      0.49      0.47       312\n",
      "          8       0.68      0.60      0.64       414\n",
      "          9       0.50      0.58      0.54       384\n",
      "\n",
      "avg / total       0.61      0.60      0.60      3583\n",
      "\n",
      "\n",
      "\n",
      "Encoded Genres:\n",
      "classic pop and rock :  0\n",
      "classical :  1\n",
      "dance and electronica :  2\n",
      "folk :  3\n",
      "hip-hop :  4\n",
      "jazz and blues :  5\n",
      "metal :  6\n",
      "pop :  7\n",
      "punk :  8\n",
      "soul and reggae :  9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t=time.time()\n",
    "feature_train,feature_test,label_train,label_test = loadInput()\n",
    "\n",
    "clf_rf = RandomForestClassifier(min_samples_split=2,min_samples_leaf=1,n_estimators=300,class_weight=\"balanced\")\n",
    "clf_rf.fit(feature_train,label_train)\n",
    "\n",
    "pred_rf = clf_rf.predict(feature_test)\n",
    "print \"Time elapsed:\",time.time()-t,\"secs.\"\n",
    "\n",
    "\n",
    "print \"confusion_matrix:\"\n",
    "print \"\"\n",
    "print confusion_matrix(label_test,pred_rf)\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "\n",
    "print \"classification_report:\"\n",
    "print classification_report(label_test,pred_rf)\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "\n",
    "print \"Encoded Genres:\" \n",
    "for index,genre in enumerate(enc.classes_):\n",
    "    print genre,\": \",index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interim Observations\n",
    "Among the algorithms used above we see that Random Forest Algorithms give the highest F1-Score of 60%.\n",
    "The next step that can be taken is:\n",
    "\n",
    "  \n",
    "    - Try ensemble methods to further optimise our model with RandomForestClassifier\n",
    "    - Try to tune hyperparameters using GridSearchCV. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OnevsRest Classifier in sklearn.\n",
    "\n",
    "Along with using ensemble methods for further optimising our model using GridSearchCV, we thought of using this with OnevsRestClassifiers. Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.\n",
    "\n",
    "After running our model through GridSearchCV we get optimisation values for the parameters.\n",
    "    - min_samples_split = 2\n",
    "    - min_samples_leaf = 2\n",
    "    - n_estimators = 300\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Training set and Test Set.\n",
      "-----------------------------------\n",
      "\n",
      "Time elapsed: 0.00197815895081 secs\n",
      "\n",
      "Size of training set:14332 samples\n",
      "Size of testing set:3583 samples\n",
      "\n",
      "\n",
      "Fitting the classifier to the training set\n",
      "Best estimator found by grid search:\n",
      "OneVsRestClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "          n_jobs=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "feature_train,feature_test,label_train,label_test = loadInput()\n",
    "\n",
    "def HPOptimizationGridSearch(feature_train,label_train):\n",
    "    print \"Fitting the classifier to the training set\"\n",
    "    param_grid = {\n",
    "             'estimator__min_samples_split': range(2,10),\n",
    "              'estimator__min_samples_leaf': range(2,10),\n",
    "              'estimator__n_estimators': range(100,400)\n",
    "              }\n",
    "    clf = GridSearchCV(OneVsRestClassifier(RandomForestClassifier(n_jobs=-1)), param_grid)\n",
    "    clf = clf.fit(feature_train,label_train)\n",
    "    print \"Best estimator found by grid search:\"\n",
    "    print clf.best_estimator_\n",
    "\n",
    "    \n",
    "#HPOptimizationGridSearch(feature_train,label_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model with Tuned Hyperparameters.\n",
    "Now, that we have find out the tuned hyperparameters using GridSearchCV. We fit our model with input data using these hyperparameters.\n",
    "\n",
    "We get a F1-score of 61% which is better than the benchmark model by 3% as mentioned in Reseach paper and 1% after we simulated the same model on our system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Training set and Test Set.\n",
      "-----------------------------------\n",
      "\n",
      "Time elapsed: 0.00377821922302 secs\n",
      "\n",
      "Size of training set:14332 samples\n",
      "Size of testing set:3583 samples\n",
      "\n",
      "\n",
      "Time elapsed: 174.244068861 secs.\n",
      "confusion_matrix:\n",
      "\n",
      "[[155  10  18  62   1  29  13  36  28  35]\n",
      " [  4 308  18   8   0  14   4   3   2   1]\n",
      " [ 18  19 275  12   2  35  11  12  12  26]\n",
      " [ 37  13   8 245   0  35   1  36  11  17]\n",
      " [  1   2  15   0  21   1   1   2   4  31]\n",
      " [ 17  38  43  28   1 224   4   8   6  21]\n",
      " [ 14   4  13   3   0   4 352   2  36   3]\n",
      " [ 28   3  23  45   1  14   5 155  10  28]\n",
      " [ 26   9  28  17   3   7  35   9 247  33]\n",
      " [ 38   3  32  24   1  19   2  39   6 220]]\n",
      "\n",
      "\n",
      "classification_report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.40      0.43       387\n",
      "          1       0.75      0.85      0.80       362\n",
      "          2       0.58      0.65      0.61       422\n",
      "          3       0.55      0.61      0.58       403\n",
      "          4       0.70      0.27      0.39        78\n",
      "          5       0.59      0.57      0.58       390\n",
      "          6       0.82      0.82      0.82       431\n",
      "          7       0.51      0.50      0.50       312\n",
      "          8       0.68      0.60      0.64       414\n",
      "          9       0.53      0.57      0.55       384\n",
      "\n",
      "avg / total       0.61      0.61      0.61      3583\n",
      "\n",
      "\n",
      "\n",
      "Encoded Genres:\n",
      "classic pop and rock :  0\n",
      "classical :  1\n",
      "dance and electronica :  2\n",
      "folk :  3\n",
      "hip-hop :  4\n",
      "jazz and blues :  5\n",
      "metal :  6\n",
      "pop :  7\n",
      "punk :  8\n",
      "soul and reggae :  9\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "\n",
    "clf_OR=OneVsRestClassifier(RandomForestClassifier(min_samples_split=2,min_samples_leaf=1,n_estimators=300,class_weight=\"balanced\"))\n",
    "\n",
    "feature_train,feature_test,label_train,label_test = loadInput()\n",
    "\n",
    "clf_OR.fit(feature_train,label_train)\n",
    "pred_OR=clf_OR.predict(feature_test)\n",
    "print \"Time elapsed:\",time.time()-t,\"secs.\"\n",
    "\n",
    "print \"confusion_matrix:\"\n",
    "print \"\"\n",
    "print confusion_matrix(label_test,pred_OR)\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "\n",
    "print \"classification_report:\"\n",
    "print classification_report(label_test,pred_OR)\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "\n",
    "print \"Encoded Genres:\" \n",
    "for index,genre in enumerate(enc.classes_):\n",
    "    print genre,\": \",index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Totally Different Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrical Analysis Model\n",
    "In this Experimental approach, what we are trying to achieve is can we predict genre using the lyrics of the songs.\n",
    "We are using requests and BeautifulSoup libraries to scrap the data out of the website.\n",
    "\n",
    "In this approach we added code to download lyrics of the songs in the dataset that were available publicly from \n",
    "[LyricsWikia](http://lyrics.wikia.com/) \n",
    "\n",
    "We downloaded around 26000 song lyrics that are available from there and store them at dataset/finalLyrics.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading song completed from lyrics.wikia.com\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment, NavigableString\n",
    "import sys, codecs, json\n",
    "\n",
    "\n",
    "def getLyrics(singer, song):\n",
    "    #Replace spaces with _\n",
    "\n",
    "    singer = singer.replace(' ', '_')\n",
    "    song = song.replace(' ', '_')\n",
    "    r = requests.get('http://lyrics.wikia.com/{0}:{1}'.format(singer,song))\n",
    "    \n",
    "    s = BeautifulSoup(r.text,'lxml')\n",
    "    #Get main lyrics holder\n",
    "    lyrics = s.find(\"div\",{'class':'lyricbox'})\n",
    "    if lyrics is None:\n",
    "        #raise ValueError(\"Song or Singer does not exist or the API does not have Lyrics\")\n",
    "        return None\n",
    "    #Remove Scripts\n",
    "    [s.extract() for s in lyrics('script')]\n",
    "\n",
    "    #Remove Comments\n",
    "    comments = lyrics.findAll(text=lambda text:isinstance(text, Comment))\n",
    "    [comment.extract() for comment in comments]\n",
    "\n",
    "    #Remove unecessary tags\n",
    "    for tag in ['div','i','b','a']:\n",
    "        for match in lyrics.findAll(tag):\n",
    "            match.replaceWithChildren()\n",
    "    #Get output as a string and remove non unicode characters and replace <br> with newlines\n",
    "    lyrics= str(lyrics).replace('\\n','').replace('<br/>',' ')\n",
    "    output=lyrics[22:-6:]\n",
    "    try:\n",
    "        return output\n",
    "    except:\n",
    "        return output.encode('utf-8')\n",
    "\n",
    "\n",
    "genres={}\n",
    "lyrics_found=0\n",
    "import codecs\n",
    "y=codecs.open(\"dataset/finalLyrics.txt\",\"w\")\n",
    "line_num=0\n",
    "with codecs.open('dataset/msd_genre_dataset.txt','r') as f:\n",
    "    for line in f:\n",
    "        if line_num<=20:\n",
    "            pass\n",
    "        else:\n",
    "            temp=line.split(',')\n",
    "            lyrics=getLyrics(temp[2],temp[3])\n",
    "            if(lyrics!=None):\n",
    "                print temp[2],temp[3]\n",
    "                if(lyrics.split(' ')[0]!='<span'):\n",
    "                    try:\n",
    "                        genres[temp[0]]+=1\n",
    "                    except:\n",
    "                        genres[temp[0]]=1\n",
    "                    lyrics_found+=1\n",
    "                    y.write(temp[0]+'**/**'+lyrics)\n",
    "                    y.write('\\n')\n",
    "        line_num+=1\n",
    "y.close()\n",
    "\n",
    "######################################################\n",
    "\n",
    "print\"Downloading song completed from lyrics.wikia.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocessing the Lyrics\n",
    "Now before we start working with our dataset, we again need to do some preprocessing over it.\n",
    "As a song lyrics may contain punctuation marks and stopwords like it, she, me, him, in e.t.c. which give us \n",
    "no information about anything we removed them using nltk library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed:2.02626033393 %.\n",
      "completed:4.05252066786 %.\n",
      "completed:6.07878100178 %.\n",
      "completed:8.10504133571 %.\n",
      "completed:10.1313016696 %.\n",
      "completed:12.1575620036 %.\n",
      "completed:14.1838223375 %.\n",
      "completed:16.2100826714 %.\n",
      "completed:18.2363430053 %.\n",
      "completed:20.2626033393 %.\n",
      "completed:22.2888636732 %.\n",
      "completed:24.3151240071 %.\n",
      "completed:26.3413843411 %.\n",
      "completed:28.367644675 %.\n",
      "completed:30.3939050089 %.\n",
      "completed:32.4201653428 %.\n",
      "completed:34.4464256768 %.\n",
      "completed:36.4726860107 %.\n",
      "completed:38.4989463446 %.\n",
      "completed:40.5252066786 %.\n",
      "completed:42.5514670125 %.\n",
      "completed:44.5777273464 %.\n",
      "completed:46.6039876803 %.\n",
      "completed:48.6302480143 %.\n",
      "completed:50.6565083482 %.\n",
      "completed:52.6827686821 %.\n",
      "completed:54.709029016 %.\n",
      "completed:56.73528935 %.\n",
      "completed:58.7615496839 %.\n",
      "completed:60.7878100178 %.\n",
      "completed:62.8140703518 %.\n",
      "completed:64.8403306857 %.\n",
      "completed:66.8665910196 %.\n",
      "completed:68.8928513535 %.\n",
      "completed:70.9191116875 %.\n",
      "completed:72.9453720214 %.\n",
      "completed:74.9716323553 %.\n",
      "completed:76.9978926893 %.\n",
      "completed:79.0241530232 %.\n",
      "completed:81.0504133571 %.\n",
      "completed:83.076673691 %.\n",
      "completed:85.102934025 %.\n",
      "completed:87.1291943589 %.\n",
      "completed:89.1554546928 %.\n",
      "completed:91.1817150267 %.\n",
      "completed:93.2079753607 %.\n",
      "completed:95.2342356946 %.\n",
      "completed:97.2604960285 %.\n",
      "completed:99.2867563625 %.\n",
      "File composition\n",
      "jazz and blues:  536\n",
      "classic pop and rock:  12363\n",
      "punk:  988\n",
      "metal:  1419\n",
      "pop:  1048\n",
      "dance and electronica:  987\n",
      "soul and reggae:  1710\n",
      "folk:  5617\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import unicodedata\n",
    "import codecs\n",
    "import string\n",
    "import pickle\n",
    "stemmer = SnowballStemmer('english')\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "data=[]\n",
    "## Processing the lyrics one by one, removing and special characters( and/or punctuation marks)\n",
    "## and finally removing all the stop words that literallly give no information about genre.\n",
    "with codecs.open('dataset/finalLyricsList.txt','r') as f:\n",
    "    lines=f.readlines()\n",
    "    count=0\n",
    "    for line in lines:\n",
    "\n",
    "        labels,features=line.split('**/**')\n",
    "        features = unicode(features, \"utf-8\")\n",
    "        features = unicodedata.normalize('NFKD',features).encode('ascii','ignore')\n",
    "        features=features.translate(None, string.punctuation)\n",
    "        features=features.lower()  \n",
    "        features=[word for word in features.split() if word not in cachedStopWords]\n",
    "        for word in features:\n",
    "            stemmer.stem(word)\n",
    "        features=\" \".join(features)\n",
    "        data.append([labels,features])\n",
    "        count+=1\n",
    "        if(count%500==0):\n",
    "            print \"completed:{} %.\".format(count/len(lines)*100.0)\n",
    "\n",
    "    genres={} \n",
    "    for line in lines:\n",
    "        labels,features=line.split('**/**')\n",
    "        try:\n",
    "            genres[labels]+=1\n",
    "        except:\n",
    "            genres[labels]=0\n",
    "    print \"File composition\"\n",
    "    for g in genres.keys():\n",
    "        print g+\": \",genres[g]\n",
    "\n",
    "#saving into serialized object\n",
    "stem_data=open(\"dataset/stemmed_text\",'wb')\n",
    "pickle.dump(data,stem_data)\n",
    "stem_data.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bag of words \n",
    "We will be using bag of words strategy over the lyrics just downloaded and processed to classify songs into various genre categories. \n",
    "\n",
    "In this Bag of words strategy, what we are doing is using TfidfVectorizer and CountVectorizer, these to calculate the frequency of the words in the lyrics and also the inverse frequency of the word in the whole dataset and assign it an importance. Each word is assigned its importance.\n",
    "\n",
    "Now, we feed this bag of words to Decision Tree, but two things to note is the decision tree overfits very easily. Plus a word can have multiple meaning related to it.So, will be taking a moderate size for min_samples_split parameter. \n",
    "\n",
    "And interestingly, we get a pretty decent F1-Score of 56% even without hypertuning anything. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Feature Ranking: \n",
      "1 feature no.46889 (0.0420229633391)\n",
      "2 feature no.35109 (0.0122767704976)\n",
      "3 feature no.15472 (0.00917726504857)\n",
      "4 feature no.41321 (0.00842699934753)\n",
      "5 feature no.66007 (0.00829721995031)\n",
      "6 feature no.61700 (0.00772061822435)\n",
      "7 feature no.10807 (0.00740004225719)\n",
      "8 feature no.32527 (0.00733699005256)\n",
      "9 feature no.41128 (0.00680181903717)\n",
      "10 feature no.14908 (0.00611062439899)\n",
      "Encoded Genres:\n",
      "classic pop and rock :  0\n",
      "dance and electronica :  1\n",
      "folk :  2\n",
      "jazz and blues :  3\n",
      "metal :  4\n",
      "pop :  5\n",
      "punk :  6\n",
      "soul and reggae :  7\n",
      "\n",
      "\n",
      "confusion_matrix:\n",
      "\n",
      "[[1781   45  453   18   69   21   25   90]\n",
      " [  66   79   42    2    6    0    1    8]\n",
      " [ 505   23  476   16   26    8   12   32]\n",
      " [  59    2   24   16    1    0    3    5]\n",
      " [  97    9   54    1  107    2    3    8]\n",
      " [  42    4   23    1    3  113   17    3]\n",
      " [  67    6   41    2   10    7   64    2]\n",
      " [ 109   11   45    4   11    4    8  144]]\n",
      "classification report:\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.71      0.68      2502\n",
      "          1       0.44      0.39      0.41       204\n",
      "          2       0.41      0.43      0.42      1098\n",
      "          3       0.27      0.15      0.19       110\n",
      "          4       0.46      0.38      0.42       281\n",
      "          5       0.73      0.55      0.63       206\n",
      "          6       0.48      0.32      0.39       199\n",
      "          7       0.49      0.43      0.46       336\n",
      "\n",
      "avg / total       0.56      0.56      0.56      4936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#loading the stemmed lyrics\n",
    "stem_data=open(\"dataset/stemmed_text\",\"r\")\n",
    "data=pickle.load(stem_data)\n",
    "\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1] ) \n",
    "    return target, features\n",
    "\n",
    "\n",
    "labels,features=targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "\n",
    "#label of the first song before encoding\n",
    "\n",
    "enc = LabelEncoder()\n",
    "target = enc.fit_transform(labels)\n",
    "\n",
    "\n",
    "feature_train,feature_test,label_train,label_test = train_test_split(features, target, test_size=0.20, random_state=42)\n",
    "vectorizer=TfidfVectorizer(sublinear_tf=True,max_df=0.5)\n",
    "feature_train=vectorizer.fit_transform(feature_train)\n",
    "feature_test=vectorizer.transform(feature_test).toarray()\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split=23)\n",
    "clf.fit(feature_train,label_train)\n",
    "pred=clf.predict(feature_test)\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print \"\"\n",
    "print \"\"\n",
    "print 'Feature Ranking: '\n",
    "for i in range(10):\n",
    "    print \"{} feature no.{} ({})\".format(i+1,indices[i],importances[indices[i]])\n",
    "\n",
    "\n",
    "print \"Encoded Genres:\" \n",
    "for index,genre in enumerate(enc.classes_):\n",
    "    print genre,\": \",index\n",
    "\n",
    "print \"\"\n",
    "print \"\"\n",
    "print \"confusion_matrix:\"\n",
    "print \"\"\n",
    "print(confusion_matrix(label_test,pred))\n",
    "print \"classification report:\"\n",
    "print \"\"\n",
    "print \"\"\n",
    "print(classification_report(label_test,pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results\n",
    "\n",
    "Following conclusions can be drawn from the above observations:\n",
    "\n",
    "    - Highest F1-score of 61% achieved using OnevsRestClassifier on RandomForestClassifier of ensemble methods.\n",
    "    \n",
    "    - Second experimental approach of lyrical analysis Model showed decent score with F1-Score of 56% \n",
    "      without hypertuning.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, there can be a lot of improvements to this project. Few improvements that come to my mind are:\n",
    "    \n",
    "1. Running the model over more data, original million song dataset with much more features.\n",
    "2. Applying PCA on features and do dimensionality reduction to reduce computation time.\n",
    "3. Use NLP and CNNs together to understand natural language and produce much better F1-Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
